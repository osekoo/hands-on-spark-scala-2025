### **Spark Labs**

Apache Spark is a powerful distributed computing framework widely used for processing large-scale data efficiently. In these labs, you will gain hands-on experience in setting up, building, and running Spark applications using **Scala** and **SBT**, as well as deploying and managing Spark clusters using **Docker**. 

The labs are designed for learners who have a basic understanding of programming and wish to deepen their knowledge of Spark's development and deployment processes. By the end of these labs, you will have built a strong foundation in developing and running Spark applications in different environments, ranging from local setups to containerized environments.


#### **Lab 1: Build a Spark Application using Scala and SBT**
https://github.com/osekoo/hands-on-spark-scala-2025/blob/develop/lab1-setup-build.md  
In this lab, you will learn to:
- Set up a local development environment for Spark using Scala and SBT.
- Build a simple word-count Spark application that processes data programmatically.
- Compile and package the application into a JAR file, ready for deployment.

This lab focuses on introducing the fundamental building blocks of Spark applications, emphasizing Scala programming and SBT project configuration.



#### **Lab 2: Run a Spark Application using Docker Containers**
https://github.com/osekoo/hands-on-spark-scala-2025/blob/develop/lab2-run-spark-app.md  
In this lab, you will learn to:
- Set up a Spark cluster using Docker and Docker Compose.
- Execute a pre-built Spark application (from Lab 1) within the Dockerized Spark environment.
- Automate cluster management using custom scripts for starting, running, and stopping the cluster.

This lab transitions from local development to containerized deployments, highlighting how to simulate real-world distributed environments.



### **Learning Objectives**
By completing these labs, you will:
1. Develop and package a Spark application in Scala.
2. Understand the basics of Spark cluster setup and execution.
3. Gain proficiency in working with Dockerized environments for Spark.
4. Build confidence in deploying and managing distributed applications.

These labs are a stepping stone toward mastering Apache Spark for real-world big data applications.
